{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AB0wKRawigW1",
        "outputId": "07033a88-67b0-456a-98c9-598a0a38b169"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Classification with numeric target\n",
            "Using Regression (target is numeric). Global stddev: 9.32\n",
            "\n",
            "=== Decision Tree Construction ===\n",
            "Algorithm: Regression\n",
            "Target: Decision\n",
            "Features: ['Outlook', 'Temp.', 'Humidity', 'Wind']\n",
            "\n",
            "if Outlook == 'Sunny':\n",
            "    if Temp. == 85:\n",
            "        return 25\n",
            "    if Temp. == 80:\n",
            "        return 30\n",
            "    if Temp. == 72:\n",
            "        return 35\n",
            "    if Temp. == 69:\n",
            "        return 38\n",
            "    if Temp. == 75:\n",
            "        return 48\n",
            "if Outlook == 'Overcast':\n",
            "    return 46.25\n",
            "if Outlook == 'Rain':\n",
            "    if Wind == 'Weak':\n",
            "        return 47.666666666666664\n",
            "    if Wind == 'Strong':\n",
            "        return 26.5\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Decision Tree Implementation\n",
        "This script implements a decision tree algorithm that can handle both:\n",
        "- Classification (using Gini impurity)\n",
        "- Regression (using standard deviation reduction)\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import math\n",
        "import numpy as np\n",
        "\n",
        "# =============================================\n",
        "# CONFIGURATION SECTION\n",
        "# =============================================\n",
        "\n",
        "# Algorithm selection - can be \"Classification\" or \"Regression\"\n",
        "algorithm = \"Classification\"\n",
        "\n",
        "# =============================================\n",
        "# DATA LOADING AND VALIDATION\n",
        "# =============================================\n",
        "\n",
        "try:\n",
        "    # Load dataset from CSV file\n",
        "    df = pd.read_csv(\"golf4.txt\")\n",
        "except FileNotFoundError:\n",
        "    raise FileNotFoundError(\"Dataset file not found. Please check the path.\")\n",
        "\n",
        "# Detect target column (flexible naming)\n",
        "target_column = None\n",
        "possible_names = ['Decision', 'Decesion', 'Class', 'Label', 'Target']\n",
        "for name in possible_names:\n",
        "    if name in df.columns:\n",
        "        target_column = name\n",
        "        break\n",
        "\n",
        "if target_column is None:\n",
        "    raise KeyError(\"Could not identify target column in dataset\")\n",
        "\n",
        "# Store feature metadata (name: dtype)\n",
        "dataset_features = {col: df[col].dtype for col in df.columns if col != target_column}\n",
        "num_features = len(dataset_features)\n",
        "\n",
        "# Validate algorithm choice against target type\n",
        "if algorithm == \"Regression\" and df[target_column].dtype == 'object':\n",
        "    raise ValueError(\"Regression requires numeric target\")\n",
        "elif algorithm == \"Classification\" and df[target_column].dtype != 'object':\n",
        "    print(\"Warning: Classification with numeric target\")\n",
        "\n",
        "# Auto-switch to regression if target is numeric\n",
        "if df[target_column].dtype != 'object':\n",
        "    algorithm = \"Regression\"\n",
        "    global_stdev = df[target_column].std(ddof=0)\n",
        "    print(f\"Using Regression (target is numeric). Global stddev: {global_stdev:.2f}\")\n",
        "\n",
        "# =============================================\n",
        "# CORE FUNCTIONS\n",
        "# =============================================\n",
        "\n",
        "def processContinousFeatures(cdf, column_name, entropy):\n",
        "    \"\"\"\n",
        "    Processes continuous/numeric features by finding optimal split threshold\n",
        "    and converting to categorical bins.\n",
        "\n",
        "    Args:\n",
        "        cdf: DataFrame containing the data\n",
        "        column_name: Name of the numeric column to process\n",
        "        entropy: Current entropy value (unused in this implementation)\n",
        "\n",
        "    Returns:\n",
        "        Modified DataFrame with binned values\n",
        "    \"\"\"\n",
        "    unique_values = sorted(cdf[column_name].unique())\n",
        "    subset_ginis = []\n",
        "    subset_red_stdevs = []\n",
        "\n",
        "    # Evaluate all possible split thresholds\n",
        "    for i in range(len(unique_values) - 1):\n",
        "        threshold = unique_values[i]\n",
        "\n",
        "        # Split data into two subsets\n",
        "        subset1 = cdf[cdf[column_name] <= threshold]\n",
        "        subset2 = cdf[cdf[column_name] > threshold]\n",
        "\n",
        "        # Calculate subset sizes\n",
        "        subset1_rows = subset1.shape[0]\n",
        "        subset2_rows = subset2.shape[0]\n",
        "        total_instances = cdf.shape[0]\n",
        "\n",
        "        if algorithm == \"Classification\":\n",
        "            # Calculate Gini impurity for classification\n",
        "            gini_subset1 = 1 - sum((subset1[target_column].value_counts()/subset1_rows)**2)\n",
        "            gini_subset2 = 1 - sum((subset2[target_column].value_counts()/subset2_rows)**2)\n",
        "            weighted_gini = (subset1_rows/total_instances)*gini_subset1 + (subset2_rows/total_instances)*gini_subset2\n",
        "            subset_ginis.append(weighted_gini)\n",
        "\n",
        "        elif algorithm == \"Regression\":\n",
        "            # Calculate standard deviation reduction for regression\n",
        "            subset1_stdev = subset1[target_column].std(ddof=0)\n",
        "            subset2_stdev = subset2[target_column].std(ddof=0)\n",
        "            weighted_stdev = (subset1_rows/total_instances)*subset1_stdev + (subset2_rows/total_instances)*subset2_stdev\n",
        "            reducted_stdev = cdf[target_column].std(ddof=0) - weighted_stdev\n",
        "            subset_red_stdevs.append(reducted_stdev)\n",
        "\n",
        "    # Select best threshold\n",
        "    if algorithm == 'Classification':\n",
        "        best_idx = subset_ginis.index(min(subset_ginis))\n",
        "    else:\n",
        "        best_idx = subset_red_stdevs.index(max(subset_red_stdevs))\n",
        "\n",
        "    best_threshold = unique_values[best_idx]\n",
        "\n",
        "    # Convert numeric column to categorical bins\n",
        "    cdf[column_name] = np.where(\n",
        "        cdf[column_name] <= best_threshold,\n",
        "        f\"<={best_threshold}\",\n",
        "        f\">{best_threshold}\"\n",
        "    )\n",
        "    return cdf\n",
        "\n",
        "def calculateEntropy(df):\n",
        "    \"\"\"\n",
        "    Calculates entropy of the target column.\n",
        "    Returns 0 for regression trees.\n",
        "\n",
        "    Args:\n",
        "        df: DataFrame containing the target column\n",
        "\n",
        "    Returns:\n",
        "        Entropy value (float)\n",
        "    \"\"\"\n",
        "    if algorithm == 'Regression':\n",
        "        return 0\n",
        "\n",
        "    counts = df[target_column].value_counts()\n",
        "    total = len(df)\n",
        "    entropy = 0\n",
        "\n",
        "    for count in counts:\n",
        "        prob = count / total\n",
        "        entropy -= prob * math.log(prob, 2)\n",
        "\n",
        "    return entropy\n",
        "\n",
        "def findDecision(ddf):\n",
        "    \"\"\"\n",
        "    Identifies the best feature to split on.\n",
        "\n",
        "    Args:\n",
        "        ddf: DataFrame containing features and target\n",
        "\n",
        "    Returns:\n",
        "        Name of the best feature to split on\n",
        "    \"\"\"\n",
        "    if algorithm == 'Regression':\n",
        "        stdev = ddf[target_column].std(ddof=0)\n",
        "\n",
        "    feature_columns = [col for col in ddf.columns if col != target_column]\n",
        "    ginis = []\n",
        "    reducted_stdevs = []\n",
        "\n",
        "    for col in feature_columns:\n",
        "        temp_df = ddf[[col, target_column]].copy()\n",
        "\n",
        "        # Process numeric features\n",
        "        if dataset_features[col] != 'object':\n",
        "            temp_df = processContinousFeatures(temp_df, col, 0)\n",
        "\n",
        "        # Calculate split quality\n",
        "        classes = temp_df[col].value_counts()\n",
        "        weighted_metric = 0\n",
        "\n",
        "        for class_val in classes.index:\n",
        "            subset = temp_df[temp_df[col] == class_val]\n",
        "            subset_size = len(subset)\n",
        "\n",
        "            if algorithm == 'Classification':\n",
        "                # Calculate Gini impurity\n",
        "                counts = subset[target_column].value_counts()\n",
        "                gini = 1 - sum((counts/subset_size)**2)\n",
        "                weighted_metric += (subset_size/len(temp_df)) * gini\n",
        "            else:\n",
        "                # Calculate standard deviation reduction\n",
        "                subset_stdev = subset[target_column].std(ddof=0)\n",
        "                weighted_metric += (subset_size/len(temp_df)) * subset_stdev\n",
        "\n",
        "        if algorithm == 'Classification':\n",
        "            ginis.append(weighted_metric)\n",
        "        else:\n",
        "            reducted_stdevs.append(stdev - weighted_metric)\n",
        "\n",
        "    # Select best feature\n",
        "    if algorithm == 'Classification':\n",
        "        best_idx = ginis.index(min(ginis))\n",
        "    else:\n",
        "        best_idx = reducted_stdevs.index(max(reducted_stdevs))\n",
        "\n",
        "    return feature_columns[best_idx]\n",
        "\n",
        "def formatRule(indent):\n",
        "    \"\"\"Helper function for tree visualization indentation\"\"\"\n",
        "    return \"    \" * indent\n",
        "\n",
        "def buildDecisionTree(df, root=0):\n",
        "    \"\"\"\n",
        "    Recursively builds and prints the decision tree.\n",
        "\n",
        "    Args:\n",
        "        df: Current subset of data\n",
        "        root: Current depth in the tree (for indentation)\n",
        "    \"\"\"\n",
        "    charForResp = \"'\" if algorithm == 'Classification' else \"\"\n",
        "\n",
        "    # Get remaining features\n",
        "    features = [col for col in df.columns if col != target_column]\n",
        "    if not features:\n",
        "        # No more features - make final decision\n",
        "        final = (df[target_column].value_counts().idxmax() if algorithm == 'Classification'\n",
        "                else df[target_column].mean())\n",
        "        print(formatRule(root) + f\"return {charForResp}{final}{charForResp}\")\n",
        "        return\n",
        "\n",
        "    # Find best split feature\n",
        "    best_feature = findDecision(df)\n",
        "    is_numeric = dataset_features[best_feature] != 'object'\n",
        "\n",
        "    # Process each value of the best feature\n",
        "    for value in df[best_feature].unique():\n",
        "        subset = df[df[best_feature] == value].drop(columns=[best_feature])\n",
        "\n",
        "        # Determine if this is a leaf node\n",
        "        is_leaf = False\n",
        "        if len(subset[target_column].unique()) == 1:\n",
        "            # All samples same class\n",
        "            decision = subset[target_column].iloc[0]\n",
        "            is_leaf = True\n",
        "        elif algorithm == 'Regression' and subset[target_column].std(ddof=0)/global_stdev < 0.4:\n",
        "            # Regression early stopping\n",
        "            decision = subset[target_column].mean()\n",
        "            is_leaf = True\n",
        "        elif len(subset.columns) == 1:  # Only target remains\n",
        "            decision = (subset[target_column].value_counts().idxmax() if algorithm == 'Classification'\n",
        "                       else subset[target_column].mean())\n",
        "            is_leaf = True\n",
        "\n",
        "        # Print decision rule\n",
        "        condition = value if is_numeric else f\"'{value}'\"\n",
        "        print(formatRule(root) + f\"if {best_feature} == {condition}:\")\n",
        "\n",
        "        if is_leaf:\n",
        "            print(formatRule(root+1) + f\"return {charForResp}{decision}{charForResp}\")\n",
        "        else:\n",
        "            buildDecisionTree(subset, root+1)\n",
        "\n",
        "# =============================================\n",
        "# EXECUTION\n",
        "# =============================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"\\n=== Decision Tree Construction ===\")\n",
        "    print(f\"Algorithm: {algorithm}\")\n",
        "    print(f\"Target: {target_column}\")\n",
        "    print(f\"Features: {list(dataset_features.keys())}\\n\")\n",
        "\n",
        "    buildDecisionTree(df)"
      ]
    }
  ]
}