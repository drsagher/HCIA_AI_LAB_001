{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### **TensorFlow Basics (HCIA-AI)**"
      ],
      "metadata": {
        "id": "xSkZPBzfRyyb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Introduction to Tensors**\n",
        "\n",
        "In TensorFlow, tensors are classified into constant and variable tensors:\n",
        "* A defined constant tensor has an immutable value and dimension while a defined variable tensor has a variable value and an immutable dimension.\n",
        "* In a neural network, a variable tensor is generally used as a matrix for storing weights and other information, and is trainable data type. A constant tensor can be used as a variable for storing hyperparameters or other structural information."
      ],
      "metadata": {
        "id": "OFKEUZIVtfeK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Creating a Constant Tensor**\n",
        "\n",
        "Methods for creating tensors:\n",
        "\n",
        "* tf.constant() - create a constant tensor\n",
        "* tf.zeros(), tf.zeros_like(), tf.ones(), tf.ones_like() creates an all-zero or all-one constant tensors.\n",
        "* tf.fill() creates a tensor with user-defined value\n",
        "* tf.random() creates a tensor with a known distribution\n",
        "* tf.convert_to_tensor() creates a list object by using NumPy and then converts it into a tensor"
      ],
      "metadata": {
        "id": "E090mB8kvRxy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "tf.constant(value,dtype=None,shape=None,name='Const', verify_shape=False);\n",
        "* value:value\n",
        "* dtype:data type\n",
        "* shape:tensor shape\n",
        "* name: name for the constant tensor\n",
        "* verify_shape : Boolean that enables verification of a shape of values. The default value is False. If verify_shape is set to True, the system checks weather the shape of value is consistent with shape. If they are inconsistent, an error is reported."
      ],
      "metadata": {
        "id": "exO8msTByfQK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**# Step1 - tf.constant()**"
      ],
      "metadata": {
        "id": "J_8ngSo207ha"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m5TP5QD-tZ7l"
      },
      "outputs": [],
      "source": [
        "# Step1 - tf.constant()\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "\n",
        "# Creating 2x2 matrix with values 1, 2,3,4\n",
        "const_a = tf.constant([1,2,3,4],shape=[2,2],dtype=tf.float32)\n",
        "\n",
        "print(const_a)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# view common attributes\n",
        "print(\"Value of const_a\",const_a.numpy())\n",
        "print(\"Data type of const_a\",const_a.dtype)\n",
        "print(\"Shape of const_a\",const_a.shape)\n",
        "print(\"device that const_a will be generated:\",const_a.device)"
      ],
      "metadata": {
        "id": "j2s5jRXGx4mi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2 tf.zeros(), tf.zeros_like(), tf.ones(), tf.ones_like()**"
      ],
      "metadata": {
        "id": "w9CyLntt0-Ea"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create an all-zero tensor\n",
        "tensor_zeros = tf.zeros(shape=(3, 3), dtype=tf.float32)\n",
        "print(\"tf.zeros():\\n\", tensor_zeros.numpy())\n",
        "\n",
        "# create an all-zero tensor with the same shape and dtype as another tensor\n",
        "tensor_zeros_like = tf.zeros_like(const_a)\n",
        "print(\"tf.zeros_like(const_a):\\n\", tensor_zeros_like.numpy())\n",
        "\n",
        "# create an all-one tensor\n",
        "tensor_ones = tf.ones(shape=(2, 4), dtype=tf.int32)\n",
        "print(\"tf.ones():\\n\", tensor_ones.numpy())\n",
        "\n",
        "# create an all-one tensor with the same shape and dtype as another tensor\n",
        "tensor_ones_like = tf.ones_like(const_a)\n",
        "print(\"tf.ones_like(const_a):\\n\", tensor_ones_like.numpy())"
      ],
      "metadata": {
        "id": "ehrKHgjS1pOp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 3 tf.fill()**"
      ],
      "metadata": {
        "id": "z6LTrtJ0td-D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a tensor with user-defined value\n",
        "fill_d = tf.fill(dims=(2, 3), value=8)\n",
        "print(\"tf.fill():\\n\", fill_d.numpy())"
      ],
      "metadata": {
        "id": "DC6nBsgQ17E6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 4 tf.random**\n",
        "\n",
        "tf.random is used to generate a tensor with a specific distribution. The common methods includes tf.random.uniform(), tf.random.normal() and tf.random.shuffle()."
      ],
      "metadata": {
        "id": "nsgEyabV1bDa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4 tf.random\n",
        "\n",
        "# tf.random.normal: Generates a tensor with random values drawn from a normal distribution.\n",
        "random_normal = tf.random.normal(shape=(2, 2), mean=0.0, stddev=1.0, dtype=tf.float32)\n",
        "print(\"tf.random.normal():\\n\", random_normal.numpy())\n",
        "\n",
        "# tf.random.uniform: Generates a tensor with random values drawn from a uniform distribution.\n",
        "random_uniform = tf.random.uniform(shape=(3, 3), minval=0, maxval=10, dtype=tf.int32)\n",
        "print(\"tf.random.uniform():\\n\", random_uniform.numpy())\n",
        "\n",
        "# tf.random.uniform: Generates a tensor with random values drawn from a uniform distribution (float).\n",
        "random_uniform_float = tf.random.uniform(shape=(2, 3), minval=0.0, maxval=1.0, dtype=tf.float32)\n",
        "print(\"tf.random.uniform(float):\\n\", random_uniform_float.numpy())\n",
        "\n",
        "# tf.random.truncated_normal: Generates a tensor with random values drawn from a truncated normal distribution.\n",
        "random_truncated_normal = tf.random.truncated_normal(shape=(4, 4), mean=0.0, stddev=1.0, dtype=tf.float32)\n",
        "print(\"tf.random.truncated_normal():\\n\", random_truncated_normal.numpy())\n",
        "\n",
        "# tf.random.shuffle: Randomly shuffles a tensor along its first dimension.\n",
        "tensor_to_shuffle = tf.constant([[1, 2], [3, 4], [5, 6]], dtype=tf.float32)\n",
        "shuffled_tensor = tf.random.shuffle(tensor_to_shuffle)\n",
        "print(\"tf.random.shuffle():\\n\", shuffled_tensor.numpy())\n",
        "\n",
        "# tf.random.stateless_normal: Generates a tensor with random values from a normal distribution given a seed.\n",
        "# Requires a seed tensor.\n",
        "seed = tf.constant([123, 456], dtype=tf.int32)\n",
        "stateless_normal = tf.random.stateless_normal(shape=(2, 2), seed=seed)\n",
        "print(\"tf.random.stateless_normal():\\n\", stateless_normal.numpy())\n",
        "\n",
        "# tf.random.stateless_uniform: Generates a tensor with random values from a uniform distribution given a seed.\n",
        "stateless_uniform = tf.random.stateless_uniform(shape=(3, 3), seed=seed, minval=0, maxval=10)\n",
        "print(\"tf.random.stateless_uniform():\\n\", stateless_uniform.numpy())\n",
        "\n",
        "# tf.random.stateless_truncated_normal: Generates a tensor with random values from a truncated normal distribution given a seed.\n",
        "stateless_truncated_normal = tf.random.stateless_truncated_normal(shape=(4, 4), seed=seed)\n",
        "print(\"tf.random.stateless_truncated_normal():\\n\", stateless_truncated_normal.numpy())\n",
        "\n",
        "# tf.random.stateless_shuffle: Randomly shuffles a tensor along its first dimension given a seed.\n",
        "# stateless_shuffled = tf.random.stateless_shuffle(tensor_to_shuffle, seed=seed)\n",
        "stateless_shuffled = tf.random.experimental.stateless_shuffle(tensor_to_shuffle, seed=seed)\n",
        "print(\"tf.random.stateless_shuffle():\\n\", stateless_shuffled.numpy())\n",
        "\n",
        "# tf.random.categorical: Draws samples from a categorical distribution.\n",
        "logits = tf.constant([[2.0, 1.0, 0.1], [0.5, 1.5, 1.0]])\n",
        "num_samples = 5\n",
        "categorical_samples = tf.random.categorical(logits, num_samples)\n",
        "print(\"tf.random.categorical():\\n\", categorical_samples.numpy())\n",
        "\n",
        "# tf.random.stateless_categorical: Draws samples from a categorical distribution given a seed.\n",
        "stateless_categorical_samples = tf.random.stateless_categorical(logits, num_samples, seed=seed)\n",
        "print(\"tf.random.stateless_categorical():\\n\", stateless_categorical_samples.numpy())\n",
        "\n",
        "# Setting a global seed for reproducible results across different calls\n",
        "tf.random.set_seed(42)\n",
        "print(\"\\nAfter setting global seed 42:\")\n",
        "random_normal_seeded = tf.random.normal(shape=(2, 2))\n",
        "print(\"tf.random.normal() with global seed:\\n\", random_normal_seeded.numpy())\n",
        "\n",
        "# Resetting the global seed to observe different results\n",
        "tf.random.set_seed(None)\n",
        "print(\"\\nAfter resetting global seed:\")\n",
        "random_normal_unseeded = tf.random.normal(shape=(2, 2))\n",
        "print(\"tf.random.normal() after resetting global seed:\\n\", random_normal_unseeded.numpy())\n"
      ],
      "metadata": {
        "id": "Mc_g7wKm3LSS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 5 Create a list object by using NumPy and then convert it into a tensor by using tf.convert_to_tensor.**\n",
        "\n",
        "This method can connvert the given value to a tensor. It converts Python objects of various types to Tensor objects\n",
        "\n",
        "tf.convert_to_tensor(value,dtype=None,dtpe_hint=None, name=None):\n",
        "\n",
        "* value : value to be converted\n",
        "* dtype : tensor data type\n",
        "* dtype_hint: optional element type for the returned tensor, used when dtype is None. In some cases, a caller may not have a dtype in mind when converting to a tensor, so dtype_int can be used as a soft preference.\n",
        "\n",
        "**Why tf.convert_to_tensor**\n",
        "\n",
        "The tf.convert_to_tensor function in TensorFlow is used to convert a given input (e.g., a Python list, NumPy array, or scalar) into a TensorFlow tensor. This is essential for integrating data with TensorFlow's computational graph and enabling operations like automatic differentiation, GPU acceleration, and model training."
      ],
      "metadata": {
        "id": "SlEH-cQa4N8y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5 tf.convert_to_tensor\n",
        "\n",
        "# create a list\n",
        "list_f = [1,2,3,4,5,6]\n",
        "\n",
        "# view the data type\n",
        "type(list_f)\n",
        "\n",
        "tensor_f = tf.convert_to_tensor(list_f,dtype=tf.float32)\n",
        "print(tensor_f)\n"
      ],
      "metadata": {
        "id": "YSp9MN5b5fQC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Creating a Variable Tensor**\n",
        "\n",
        "In TensorFlow, variables are created and tracked via the tf.Variable class. A tf.Variable represents a tensor whose value can be changed by running ops on it. Specific ops allow you to read and modify the values."
      ],
      "metadata": {
        "id": "H-xWXqsB6n2q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# To create a variable, provide an initial value\n",
        "\n",
        "var_1 = tf.Variable(tf.ones([2,3]))\n",
        "var_1"
      ],
      "metadata": {
        "id": "Aapccs5t8Wrb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the variable value\n",
        "print(\"value of var_1:\", var_1.read_value())\n",
        "\n",
        "# assign a variable value\n",
        "var_value_1 = [[1,2,3],[4,5,6]]\n",
        "var_1.assign(var_value_1)\n",
        "\n",
        "# print the new value\n",
        "print(\"new value of var_1:\", var_1.read_value())"
      ],
      "metadata": {
        "id": "l93BxS-Y8dpz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# add value to this variable\n",
        "var_1.assign_add(tf.ones([2,3]))\n",
        "print(\"new value of var_1:\", var_1.read_value())\n",
        "\n",
        "var_1"
      ],
      "metadata": {
        "id": "Uv54v9Me7M5U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tensor Slicing**\n",
        "\n",
        "* slicing is used to create mini-batches for training neural networks, isolate specific channels in image data, or extract time steps in sequence models like RNNs.\n",
        "\n",
        "* slicing provides flexibility in handling complex, high-dimensional data (e.g., images, videos, or time-series), making it easier to implement algorithms that require specific data views or transformations. By avoiding unnecessary data copying and enabling precise access to tensor elements, slicing enhances memory efficiency and computational speed, which are crucial for large-scale machine learning tasks\n",
        "\n",
        "Major slicing methods include:\n",
        "* [start:end] : extract a data slice from the start position to the end position of a tensor\n",
        "* [start:end:step] or [::step] : extracts a data slice at an internal of step from the start position to the end position of a tensor\n",
        "* [::-1] : slice data from the last element\n",
        "* '...': indicates a data slice of any length\n"
      ],
      "metadata": {
        "id": "NiBdYXw88ovL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a 4 dimensional tensor. The tensor contains four images. the size of each image is 100 x 100 x 3\n",
        "\n",
        "tensor_h = tf.random.normal([4,100,100,3])\n",
        "tensor_h"
      ],
      "metadata": {
        "id": "hqXyxB8IGNsT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the first image\n",
        "\n",
        "tensor_h[0,:,:,:]"
      ],
      "metadata": {
        "id": "ybFCbkoBGolL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extract one slice every two images\n",
        "\n",
        "tensor_h[::2,...]"
      ],
      "metadata": {
        "id": "stsqyxtTG2sS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Slice data from last element\n",
        "\n",
        "tensor_h[::-1]"
      ],
      "metadata": {
        "id": "0l8KFNDEHDuC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Indexing**\n",
        "\n",
        "The basic format of an index is a[d1][d2][d3]\n",
        "\n",
        "If the indexes to be extracted are nonconsecutive, tf.gather and tf.gather_nd are commonly used for data extraction in TensorFlow"
      ],
      "metadata": {
        "id": "S2OZpg1SHxgz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtain the pixel in the [20,40] position in the second channel of the first image\n",
        "\n",
        "tensor_h[0][19][39][1]"
      ],
      "metadata": {
        "id": "P1HCic1MIDS6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**To extract data from a particular dimension:**\n",
        "\n",
        "*tf.gather(params, indices, axis=None):*\n",
        "\n",
        "* *params* :input tensor\n",
        "* *indices* : index of the data to be extracted\n",
        "* *axis* : dimension of the data to be extracted"
      ],
      "metadata": {
        "id": "u6gU1J99J2tD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the first, second, and fourth images from tensor_h([4,100,100,3])\n",
        "\n",
        "indices = [0,1,3]\n",
        "\n",
        "tf.gather(tensor_h,indices=indices,axis=0)"
      ],
      "metadata": {
        "id": "fWfcXddRJS4T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "tf.gather_nd allows data extraction from multiple dimensions:\n",
        "\n",
        "* params : input tensor\n",
        "* indices : index of the data to be extracted. generally, this is a multidimensional list."
      ],
      "metadata": {
        "id": "Bdb1c5BeJ9mj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the pixel in [1,1] in the first dimension of the first image and pixel in [2,2] in the first dimension of the second image in tensor_h([4,100,100,3])\n",
        "\n",
        "indices = [[0,1,1,0],[1,2,2,0]]\n",
        "\n",
        "tf.gather_nd(tensor_h,indices=indices)"
      ],
      "metadata": {
        "id": "UgCLdOLeKN4T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dimension Display**\n",
        "\n",
        "Note:\n",
        "* .shape and .get_shape() return TensorShape objects, while tf.shape(x) return Tensor objects"
      ],
      "metadata": {
        "id": "jQVJSLhdL_B7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "const_d_1 = tf.constant([[1,2,3,4]],shape=[2,2],dtype=tf.float32)\n",
        "print(const_d_1)\n",
        "\n",
        "print(const_d_1.shape)\n",
        "print(const_d_1.get_shape())\n",
        "\n",
        "# The output is a tensor. The value of the tensor indicates the size of the tensor dimension to be displayed\n",
        "print(tf.shape(const_d_1))"
      ],
      "metadata": {
        "id": "o-wF3AjsMBlz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dimension Reshaping\n",
        "\n",
        "tf.reshape(tensor, shape, name=None):\n",
        "* tensor: input tensor\n",
        "* shape : shape of the reshaped tensor"
      ],
      "metadata": {
        "id": "_ITXiKbTNCkW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reshape_1 = tf.constant([[1,2,3],[4,5,6]])\n",
        "\n",
        "print(reshape_1)\n",
        "\n",
        "print(tf.reshape(reshape_1,shape=[3,2]))"
      ],
      "metadata": {
        "id": "QfYHCCH1NSo6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dimnsion Expansion**\n",
        "\n",
        "tf.expand_dims(input, axix,name=None)\n",
        "* input : input tensor\n",
        "* axix : adds a dimension after the axis dimension. Given an input of D dimensions, axis must be in range[-(d+1),D] (inclusive). A negative value indicates the reverse order."
      ],
      "metadata": {
        "id": "lAtRTgE8N0jo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate a 100x100x3 tensor to reppresent a 100x100 three-channel color image.\n",
        "\n",
        "expand_sample_1 = tf.random.normal([100,100,3],seed=1)\n",
        "\n",
        "print(\"origional data size : \", expand_sample_1.shape)\n",
        "\n",
        "print(\"add a dimension (axix=0) before the first dimension : \", tf.expand_dims(expand_sample_1, axis=0).shape)\n",
        "\n",
        "print(\"add a dimension (axix=1) before the second dimension : \", tf.expand_dims(expand_sample_1, axis=1).shape)\n",
        "\n",
        "print(\"add a dimension (axix=-1) before the last dimension : \", tf.expand_dims(expand_sample_1, axis=-1).shape)\n"
      ],
      "metadata": {
        "id": "a5uY9NCoOckb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dimension Squeezing**\n",
        "\n",
        "tf.squeeze(input, axis=None, name=None)\n",
        "\n",
        "This method is used to remove dismensions of size 1 from the shape of a tensor.\n",
        "* input : input tensor\n",
        "* axis: if you do not want to remove all size dimensions, remove specific size 1 dimensions by specifying axis"
      ],
      "metadata": {
        "id": "6Eb0Ot0PgFqV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate a 100x100x3 tensor\n",
        "\n",
        "orig_ample_1 = tf.random.normal([1,100,100,3])\n",
        "\n",
        "print(\"origional data size : \", orig_ample_1.shape)\n",
        "\n",
        "sqeezed_sample_1 = tf.squeeze(orig_ample_1)\n",
        "\n",
        "print(\"squeezed data size : \", sqeezed_sample_1.shape)\n",
        "\n",
        "# The dimension of sqeeze_sampme_2 is [1,2,1,3,1,1]\n",
        "sqeezed_sample_2 = tf.random.normal([1,2,1,3,1,1])\n",
        "t_1=tf.squeeze(sqeezed_sample_2)\n",
        "print(\"t_1.shape : \",t_1.shape)\n",
        "\n",
        "# Remove a specific dimenion\n",
        "# t is a tensor of shape [1,2,1,3,1,1]\n",
        "\n",
        "t_1_new = tf.squeeze(sqeezed_sample_2,[2,2])\n",
        "print(\"t_1_new.shape : \",t_1_new.shape)\n"
      ],
      "metadata": {
        "id": "_6HjlsG1gm-F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Transpose**\n",
        "\n",
        "tf.transpose(a,perm=None, conjugate=False,name='transpose')\n",
        "* a: input tensor\n",
        "* perm: permutation of the dimensions of a, generally used to transpose high-dimensional arrays\n",
        "* conjugate: conjugate tranpose\n",
        "* name: name of the operation\n"
      ],
      "metadata": {
        "id": "nNak4nPlinq1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Low-dimensional transposition in simple. Input the tensor to be transposed by calling tf.transpose\n",
        "\n",
        "transpose_sample_1 = tf.constant([1,2,3,4,5,6],shape=[2,3])\n",
        "print(\"Origional data size\", transpose_sample_1.shape)\n",
        "\n",
        "transposed_sample_1 = tf.transpose(transpose_sample_1)\n",
        "print(\"Tranposed data size\",transposed_sample_1.shape)"
      ],
      "metadata": {
        "id": "_XIWoOPpjX2V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The perm parameter is required for transposing high -dimensional data. perm indicates the permutation of the dimensions of the input tensor.\n",
        "For a three-dimensional tensor, its origional dimension permutation is [0,1,2] (perm), including the length, width, and height of the high-dimensional data, respectively.\n",
        "By changing the value  squeeze in per, we can transpose the corresponnding dimension of the data."
      ],
      "metadata": {
        "id": "lgtH0snukL7N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate a 4x100x200x3 tensor to represent four 100x200 three-channel color images\n",
        "\n",
        "trans_sample_2 = tf.random.normal([4,100,200,3])\n",
        "print(\"Origional data size : \", trans_sample_2.shape)\n",
        "\n",
        "# Exchange the length and width of the four images. The value of perm is changed from [0,1,2,3] to [0,2,1,3]\n",
        "\n",
        "transposed_sample_2 = tf.transpose(trans_sample_2,perm=[0,2,1,3])\n",
        "print(\"Transposed data size : \", transposed_sample_2.shape)\n",
        "#"
      ],
      "metadata": {
        "id": "q6aNWROqlMHk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Broadcast (broadcast_to)**\n",
        "\n",
        "broadcast_to is used to broadcast data from a low dimension to a high dimension.\n",
        "*tf.broadcast_to(input,shape,name=None)*\n",
        "\n",
        "* input : input tensor\n",
        "* shape : size of the output tensor"
      ],
      "metadata": {
        "id": "GeG9QFdHmTtU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "broadcast_sample_1 = tf.constant([1,2,3,4,5,6])\n",
        "print(\"Origional data size : \", broadcast_sample_1.numpy())\n",
        "\n",
        "broadcasted_sample_1 = tf.broadcast_to(broadcast_sample_1,shape=[4,6])\n",
        "print(\"Broadcasted data size : \", broadcasted_sample_1.numpy())"
      ],
      "metadata": {
        "id": "S6KExq-4m2m0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# During the operation, if two arrays have different shapes, TensorFlow automatically triggers the broadcast mechanism as NumPy does\n",
        "\n",
        "a=tf.constant([[0,0,0],[10,10,10],[20,20,20],[30,30,30]])\n",
        "b=tf.constant([1,2,3])\n",
        "\n",
        "print(a+b)"
      ],
      "metadata": {
        "id": "s8jYRnbMnSd8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Arithmatic Operations of Tensors**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YON--vQ0oUAU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Arithmatics Operators**\n",
        "\n",
        "Arithmatic operations include addition(tf.add), subtraction(tf.subtract), multiplication(tf.multiply), division(tf.divide), logarithm(tf.math.log), and powers(tf.pow).\n",
        "\n"
      ],
      "metadata": {
        "id": "zJsSGY4SoflD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Addition\n",
        "tensor_a = tf.constant([1, 2, 3], dtype=tf.float32)\n",
        "tensor_b = tf.constant([4, 5, 6], dtype=tf.float32)\n",
        "addition_result = tf.add(tensor_a, tensor_b)\n",
        "print(\"Addition:\", addition_result.numpy())\n",
        "\n",
        "# Subtraction\n",
        "subtraction_result = tf.subtract(tensor_b, tensor_a)\n",
        "print(\"Subtraction:\", subtraction_result.numpy())\n",
        "\n",
        "# Multiplication (element-wise)\n",
        "multiplication_result = tf.multiply(tensor_a, tensor_b)\n",
        "print(\"Multiplication:\", multiplication_result.numpy())\n",
        "\n",
        "# Division (element-wise)\n",
        "tensor_c = tf.constant([10.0, 20.0, 30.0], dtype=tf.float32)\n",
        "tensor_d = tf.constant([2.0, 5.0, 10.0], dtype=tf.float32)\n",
        "division_result = tf.divide(tensor_c, tensor_d)\n",
        "print(\"Division:\", division_result.numpy())\n",
        "\n",
        "# Powers\n",
        "tensor_f = tf.constant([2.0, 3.0, 4.0], dtype=tf.float32)\n",
        "tensor_g = tf.constant([2.0, 3.0, 0.5], dtype=tf.float32)\n",
        "powers_result = tf.pow(tensor_f, tensor_g)\n",
        "print(\"Powers:\", powers_result.numpy())\n",
        "\n",
        "# Logarithm of a tensor\n",
        "positive_tensor = tf.constant([[1.0, 2.0], [3.0, 4.0]], dtype=tf.float32)\n",
        "log_positive_tensor = tf.math.log(positive_tensor)\n",
        "print(\"Logarithm of a tensor with positive elements:\\n\", log_positive_tensor.numpy())"
      ],
      "metadata": {
        "id": "gRk0i2yopjSk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Matrix Multiplication**"
      ],
      "metadata": {
        "id": "ka0pYZwDqWcr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Matrix Multiplication\n",
        "\n",
        "# 2D Tensors\n",
        "tensor_2d_a = tf.constant([[1, 2], [3, 4]], dtype=tf.float32)\n",
        "tensor_2d_b = tf.constant([[5, 6], [7, 8]], dtype=tf.float32)\n",
        "matmul_2d = tf.matmul(tensor_2d_a, tensor_2d_b)\n",
        "print(\"Matrix multiplication of 2D tensors:\\n\", matmul_2d.numpy())\n",
        "\n",
        "# 3D Tensors\n",
        "# Shape: (batch_size, rows, columns)\n",
        "tensor_3d_a = tf.constant([[[1, 2], [3, 4]], [[5, 6], [7, 8]]], dtype=tf.float32) # Shape (2, 2, 2)\n",
        "tensor_3d_b = tf.constant([[[9, 10], [11, 12]], [[13, 14], [15, 16]]], dtype=tf.float32) # Shape (2, 2, 2)\n",
        "matmul_3d = tf.matmul(tensor_3d_a, tensor_3d_b)\n",
        "print(\"Matrix multiplication of 3D tensors:\\n\", matmul_3d.numpy())\n",
        "\n",
        "# 4D Tensors\n",
        "# Shape: (batch_size, channels, rows, columns) or (batch_size, height, width, channels) - depends on convention\n",
        "# Let's use (batch_size, channels, rows, columns)\n",
        "tensor_4d_a = tf.constant([[[[1, 2], [3, 4]], [[5, 6], [7, 8]]]], dtype=tf.float32) # Shape (1, 2, 2, 2)\n",
        "tensor_4d_b = tf.constant([[[[9, 10], [11, 12]], [[13, 14], [15, 16]]]], dtype=tf.float32) # Shape (1, 2, 2, 2)\n",
        "matmul_4d = tf.matmul(tensor_4d_a, tensor_4d_b)\n",
        "print(\"Matrix multiplication of 4D tensors:\\n\", matmul_4d.numpy())"
      ],
      "metadata": {
        "id": "ElmZIgGrrlM8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tensor Statistics Collection**\n",
        "\n",
        "Methods for collecting tensor statistics include:\n",
        "* tf.reduce_min/max/mean() : calculates minimum, maximum, and mean values.\n",
        "* tf.argmax()/tf.argmin(): calculates the position of the maximum and minimum values\n",
        "* tf.equal(): check weather two tensors are equal by element\n",
        "* tf.unique(): removes duplicate elements from a tensor\n",
        "* tf.nn.in_top_k(prediction, target, K): calculates weather the predicted value is equal to the actual value and returns a tensor of the Boolean type\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "SqYG8-cpsASb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# tf.reduce_min/max/mean\n",
        "tensor_stats = tf.constant([[1, 2, 3], [4, 5, 6]], dtype=tf.float32)\n",
        "min_value = tf.reduce_min(tensor_stats)\n",
        "max_value = tf.reduce_max(tensor_stats)\n",
        "mean_value = tf.reduce_mean(tensor_stats)\n",
        "print(\"Tensor for statistics:\", tensor_stats.numpy())\n",
        "print(\"tf.reduce_min:\", min_value.numpy())\n",
        "print(\"tf.reduce_max:\", max_value.numpy())\n",
        "print(\"tf.reduce_mean:\", mean_value.numpy())\n",
        "\n",
        "# You can also reduce along specific axes\n",
        "min_along_axis_0 = tf.reduce_min(tensor_stats, axis=0)\n",
        "max_along_axis_1 = tf.reduce_max(tensor_stats, axis=1)\n",
        "print(\"tf.reduce_min along axis 0:\", min_along_axis_0.numpy())\n",
        "print(\"tf.reduce_max along axis 1:\", max_along_axis_1.numpy())\n",
        "\n",
        "# tf.argmax()/tf.argmin()\n",
        "tensor_arg = tf.constant([[1, 5, 3], [4, 2, 6]], dtype=tf.float32)\n",
        "argmax_value = tf.argmax(tensor_arg) # Default axis is the last axis\n",
        "argmin_value = tf.argmin(tensor_arg) # Default axis is the last axis\n",
        "print(\"Tensor for argmax/argmin:\", tensor_arg.numpy())\n",
        "print(\"tf.argmax (along last axis):\", argmax_value.numpy())\n",
        "print(\"tf.argmin (along last axis):\", argmin_value.numpy())\n",
        "\n",
        "# Argmax/argmin along specific axes\n",
        "argmax_axis_0 = tf.argmax(tensor_arg, axis=0)\n",
        "argmin_axis_1 = tf.argmin(tensor_arg, axis=1)\n",
        "print(\"tf.argmax along axis 0:\", argmax_axis_0.numpy())\n",
        "print(\"tf.argmin along axis 1:\", argmin_axis_1.numpy())\n",
        "\n",
        "# tf.equal()\n",
        "tensor_eq1 = tf.constant([[1, 2], [3, 4]])\n",
        "tensor_eq2 = tf.constant([[1, 5], [3, 4]])\n",
        "equal_result = tf.equal(tensor_eq1, tensor_eq2)\n",
        "print(\"Tensor 1 for equality:\", tensor_eq1.numpy())\n",
        "print(\"Tensor 2 for equality:\", tensor_eq2.numpy())\n",
        "print(\"tf.equal:\", equal_result.numpy())\n",
        "\n",
        "# tf.unique()\n",
        "tensor_unique = tf.constant([1, 5, 2, 5, 3, 1, 4])\n",
        "unique_elements, original_indices = tf.unique(tensor_unique)\n",
        "print(\"Tensor for unique:\", tensor_unique.numpy())\n",
        "print(\"tf.unique elements:\", unique_elements.numpy())\n",
        "print(\"tf.unique original indices:\", original_indices.numpy())\n",
        "\n",
        "# For a 2D tensor, tf.unique works on the flattened tensor\n",
        "tensor_unique_2d = tf.constant([[1, 5], [2, 5], [3, 1], [4, 4]])\n",
        "unique_elements_2d, original_indices_2d = tf.unique(tf.reshape(tensor_unique_2d, [-1]))\n",
        "print(\"Tensor for unique (2D, flattened):\", tensor_unique_2d.numpy())\n",
        "print(\"tf.unique elements (flattened):\", unique_elements_2d.numpy())\n",
        "print(\"tf.unique original indices (flattened):\", original_indices_2d.numpy())\n",
        "\n",
        "\n",
        "# tf.nn.in_top_k(predictions, targets, k)\n",
        "# This function is typically used in classification tasks to check if the true target is among the top K predictions.\n",
        "# Note: tf.nn.in_top_k is deprecated in TensorFlow 2.0+. Use tf.math.in_top_k instead.\n",
        "\n",
        "# Example with tf.math.in_top_k\n",
        "predictions = tf.constant([[0.1, 0.9, 0.2], [0.8, 0.2, 0.0]], dtype=tf.float32) # Example probabilities or logits for 2 examples and 3 classes\n",
        "targets = tf.constant([1, 0], dtype=tf.int32) # True class indices for the 2 examples\n",
        "k_value = 1 # Check if the top 1 prediction is correct\n",
        "print(k_value)\n"
      ],
      "metadata": {
        "id": "IgwTVfFbuLBk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dimensions"
      ],
      "metadata": {
        "id": "9ji9afYhwby0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dimensions-based Arithmetic Operations**\n",
        "\n",
        "In TensorFlow, operations such as tf.reduce_* tensor dimensions. These operations can be performed on the dimension elements of a tensor, for example, calculating the mean value by row and calculating a product of all elements in the tensor.\n",
        "Common operations include tf.reduce_sum(addition), tf.reduce_prod(multiplication), tf.reduce_min(minimum), tf.reduce_max(maximum), tf.reduce_mean(mean), tf.reduce_all(logical AND), tf.reduce_any(logical OR), tf.reduce_logsumexp(log(sum(exp))).\n"
      ],
      "metadata": {
        "id": "2E6yA9CHxvez"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a simple tensor\n",
        "tensor_reduce = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]], dtype=tf.float32)\n",
        "print(\"Original tensor:\\n\", tensor_reduce.numpy())\n",
        "\n",
        "# tf.reduce_sum (addition)\n",
        "sum_all = tf.reduce_sum(tensor_reduce)\n",
        "print(\"\\ntf.reduce_sum (all elements):\", sum_all.numpy())\n",
        "sum_axis_0 = tf.reduce_sum(tensor_reduce, axis=0) # Sum along columns\n",
        "print(\"tf.reduce_sum (axis=0):\", sum_axis_0.numpy())\n",
        "sum_axis_1 = tf.reduce_sum(tensor_reduce, axis=1) # Sum along rows\n",
        "print(\"tf.reduce_sum (axis=1):\", sum_axis_1.numpy())\n",
        "\n",
        "# tf.reduce_prod (multiplication)\n",
        "prod_all = tf.reduce_prod(tensor_reduce)\n",
        "print(\"\\ntf.reduce_prod (all elements):\", prod_all.numpy())\n",
        "prod_axis_0 = tf.reduce_prod(tensor_reduce, axis=0) # Product along columns\n",
        "print(\"tf.reduce_prod (axis=0):\", prod_axis_0.numpy())\n",
        "prod_axis_1 = tf.reduce_prod(tensor_reduce, axis=1) # Product along rows\n",
        "print(\"tf.reduce_prod (axis=1):\", prod_axis_1.numpy())\n",
        "\n",
        "# tf.reduce_min (minimum)\n",
        "min_all = tf.reduce_min(tensor_reduce)\n",
        "print(\"\\ntf.reduce_min (all elements):\", min_all.numpy())\n",
        "min_axis_0 = tf.reduce_min(tensor_reduce, axis=0) # Minimum along columns\n",
        "print(\"tf.reduce_min (axis=0):\", min_axis_0.numpy())\n",
        "min_axis_1 = tf.reduce_min(tensor_reduce, axis=1) # Minimum along rows\n",
        "print(\"tf.reduce_min (axis=1):\", min_axis_1.numpy())\n",
        "\n",
        "# tf.reduce_max (maximum)\n",
        "max_all = tf.reduce_max(tensor_reduce)\n",
        "print(\"\\ntf.reduce_max (all elements):\", max_all.numpy())\n",
        "max_axis_0 = tf.reduce_max(tensor_reduce, axis=0) # Maximum along columns\n",
        "print(\"tf.reduce_max (axis=0):\", max_axis_0.numpy())\n",
        "max_axis_1 = tf.reduce_max(tensor_reduce, axis=1) # Maximum along rows\n",
        "print(\"tf.reduce_max (axis=1):\", max_axis_1.numpy())\n",
        "\n",
        "# tf.reduce_mean (mean)\n",
        "mean_all = tf.reduce_mean(tensor_reduce)\n",
        "print(\"\\ntf.reduce_mean (all elements):\", mean_all.numpy())\n",
        "mean_axis_0 = tf.reduce_mean(tensor_reduce, axis=0) # Mean along columns\n",
        "print(\"tf.reduce_mean (axis=0):\", mean_axis_0.numpy())\n",
        "mean_axis_1 = tf.reduce_mean(tensor_reduce, axis=1) # Mean along rows\n",
        "print(\"tf.reduce_mean (axis=1):\", mean_axis_1.numpy())\n",
        "\n",
        "# tf.reduce_all (logical AND) - requires boolean tensor\n",
        "tensor_bool = tf.constant([[True, False, True], [True, True, False]], dtype=tf.bool)\n",
        "print(\"\\nOriginal boolean tensor:\\n\", tensor_bool.numpy())\n",
        "all_all = tf.reduce_all(tensor_bool)\n",
        "print(\"tf.reduce_all (all elements):\", all_all.numpy())\n",
        "all_axis_0 = tf.reduce_all(tensor_bool, axis=0) # AND along columns\n",
        "print(\"tf.reduce_all (axis=0):\", all_axis_0.numpy())\n",
        "all_axis_1 = tf.reduce_all(tensor_bool, axis=1) # AND along rows\n",
        "print(\"tf.reduce_all (axis=1):\", all_axis_1.numpy())\n",
        "\n",
        "# tf.reduce_any (logical OR) - requires boolean tensor\n",
        "any_all = tf.reduce_any(tensor_bool)\n",
        "print(\"\\ntf.reduce_any (all elements):\", any_all.numpy())\n",
        "any_axis_0 = tf.reduce_any(tensor_bool, axis=0) # OR along columns\n",
        "print(\"tf.reduce_any (axis=0):\", any_axis_0.numpy())\n",
        "any_axis_1 = tf.reduce_any(tensor_bool, axis=1) # OR along rows\n",
        "print(\"tf.reduce_any (axis=1):\", any_axis_1.numpy())\n",
        "\n",
        "# tf.reduce_logsumexp (log(sum(exp)))\n",
        "# This is often used in the log-probability calculation, e.g., for softmax.\n",
        "tensor_logsumexp = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]], dtype=tf.float32)\n",
        "print(\"\\nOriginal tensor for logsumexp:\\n\", tensor_logsumexp.numpy())\n",
        "logsumexp_all = tf.reduce_logsumexp(tensor_logsumexp)\n",
        "print(\"tf.reduce_logsumexp (all elements):\", logsumexp_all.numpy())\n",
        "logsumexp_axis_0 = tf.reduce_logsumexp(tensor_logsumexp, axis=0) # LogSumExp along columns\n",
        "print(\"tf.reduce_logsumexp (axis=0):\", logsumexp_axis_0.numpy())\n",
        "logsumexp_axis_1 = tf.reduce_logsumexp(tensor_logsumexp, axis=1) # LogSumExp along rows\n",
        "print(\"tf.reduce_logsumexp (axis=1):\", logsumexp_axis_1.numpy())"
      ],
      "metadata": {
        "id": "FvmfMaWsyhHr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Vbt_esVEy4tz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tensor Concatenation**\n",
        "\n",
        "In TensorFlow, tensor concatenation operations include:\n",
        "* tf.concat(): concatenates tensors along one dimesnions remain unchanged\n",
        "* tf.stack():stacks the tensor list of rank R into a tensor of rank (R+1). Dimensins are changed after stacking.\n",
        "\n",
        "*tf.concat(values, axis, name='concat')*\n",
        "\n",
        "* values: input tensor\n",
        "* axis: dimension along which to concatenate\n",
        "* name: name for operation\n"
      ],
      "metadata": {
        "id": "j9vRLw5QzDUz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "concat_sample_1=tf.random.normal([4,100,100,3])\n",
        "concat_sample_2=tf.random.normal([40,100,100,3])\n",
        "\n",
        "# Print origional tensor\n",
        "print(\"Origional data size\", concat_sample_1.shape,concat_sample_2.shape)\n",
        "\n",
        "concat_sample_1 = tf.concat([concat_sample_1,concat_sample_2],axis=0)\n",
        "\n",
        "print(\"Concatenated data size\", concat_sample_1.shape)"
      ],
      "metadata": {
        "id": "Gt6WrUU-z9Ty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A dimension is added to an origional matrix in the same way. axis determines the position where the dimension is added.\n",
        "\n",
        "*tf.stack(values, axis=0, name='stack')*\n",
        "* values: a list of tensor objects with the same shape and type\n",
        "* axis: axis to stack along\n",
        "* name: name for the operation"
      ],
      "metadata": {
        "id": "n-D2FQFg0n9M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stack_sample_1 = tf.random.normal([4, 100, 100, 3])\n",
        "stack_sample_3 = tf.random.normal([4, 100, 100, 3])  # Shape matches stack_sample_1\n",
        "\n",
        "# Print original tensor shapes\n",
        "print(\"Original data size:\", stack_sample_1.shape, stack_sample_3.shape)\n",
        "\n",
        "# Stack along axis=0 (adds a new dimension at the beginning)\n",
        "stacked_sample = tf.stack([stack_sample_1, stack_sample_3], axis=0)  # Fixed variable name\n",
        "\n",
        "print(\"Stacked data size:\", stacked_sample.shape)"
      ],
      "metadata": {
        "id": "prgTH6IC1Isz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tensor Splitting**\n",
        "\n",
        "In TensorFlow, tensor splitting operations include:\n",
        "* tf.unstack(): unpacks tensors along the specific dimension\n",
        "* tf.split(): splits a tensor into a list of sub tensors based on specific dimensions\n",
        "\n",
        "Compared with tf.unstack(), tf.split() is more flexible.\n",
        "\n",
        "*tf.unstack(value,num=None, axis=0, name='unstack')*\n",
        "\n",
        "* value: input tensor\n",
        "* num: outputs a list containing num elements. num must be equal to the number of elements in the specified dimension. Generally, this parameter is ignored.\n",
        "* axis: axis to unstack along\n",
        "* name: name for the operation\n",
        "\n"
      ],
      "metadata": {
        "id": "o9X47EL_2ZTj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# unpack data along the first dimension and output data in a list\n",
        "tf.unstack(stacked_sample,axis=0)"
      ],
      "metadata": {
        "id": "O-PmlHa53-VK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*tf.split(value,num_or_size_splits, axis=0)*\n",
        "\n",
        "* value: input tensor\n",
        "* num_or_size_splits: number of splits\n",
        "* axis: dimension along which to split\n",
        "\n",
        "tf.split() can be split in either of the following ways:\n",
        "* If num_or_size_splits is an integer, the tensor is evenly split into several small tensors along the axis=D\n",
        "* if num_or_size_splits is a vector, the tensor is split into several smaller tensors based on the element values of the vector along the axis=D dimension\n"
      ],
      "metadata": {
        "id": "qJWLRg6o4RHL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "split_sample_1=tf.random.normal([10,100,100,3])\n",
        "\n",
        "print(\"Origional data size\", split_sample_1.shape)\n",
        "\n",
        "splited_sample_1 = tf.split(split_sample_1, num_or_size_splits=5, axis=0)\n",
        "\n",
        "print(\"If num_or_size_splits=5 the size of Splited data is: \", np.shape(splited_sample_1))\n",
        "\n",
        "splited_sample_2 = tf.split(split_sample_1, num_or_size_splits=[3,5,2], axis=0)\n",
        "\n",
        "print(\"If num_or_size_splits=[3,5,2] the size of Splited data is: \", np.shape(splited_sample_2[0]), np.shape(splited_sample_2[1]), np.shape(splited_sample_2[2]))"
      ],
      "metadata": {
        "id": "zqwiJ2b65VwD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tensor Sorting**\n",
        "\n",
        "In TensorFlow, tensor sorting operations include:\n",
        "* tf.sort(): sorts tensors in ascending or descending order and returns the sorted tensors.\n",
        "* tf.argsort(): sorts tensors in ascending or descending order and returns indices\n",
        "* tf.nn.top_k(): returns the k largest values\n",
        "\n",
        "*tf.sort/argsort(input,direction, axis):*\n",
        "\n",
        "* input: input tensor\n",
        "* direction: direction in which to sort the values. The value can be ascending or descending. The default value is ascending.\n",
        "* axis: axis along which to sort the default value is -1, which sorts the last axis."
      ],
      "metadata": {
        "id": "vKgrz72a9VKq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sort_sample_1 = tf.random.shuffle(tf.range(10))\n",
        "\n",
        "print(\"Input tensor\", sort_sample_1.numpy())\n",
        "\n",
        "sorted_sample_1 = tf.sort(sort_sample_1, direction=\"ASCENDING\")\n",
        "print(\"Ascending order\", sorted_sample_1.numpy())\n",
        "\n",
        "sorted_sample_2 = tf.sort(sort_sample_1, direction=\"DESCENDING\")\n",
        "print(\"Descending order\", sorted_sample_2.numpy())"
      ],
      "metadata": {
        "id": "cDiouAFy-7lb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*tf.nn.top_k(input, K, sorted=TRUE):*\n",
        "\n",
        "* input: input tensor\n",
        "* K : k largest values to be output and their indices\n",
        "* sorted: sorted=TRUE indicates in ascending order. sorted=FALSE indicates in descending order.\n",
        "\n",
        "Two tensors are returned:\n",
        "* values: k largest values in each row\n",
        "* indices: indices of values within the last dimension of input"
      ],
      "metadata": {
        "id": "_HinZL3F_cgS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ascending order\n",
        "values, index = tf.nn.top_k(sort_sample_1, 5)\n",
        "print(\"Input tensor\", sort_sample_1.numpy())\n",
        "print(\"K largest values in ascending order: \", values.numpy())\n",
        "print(\"Indices of the largest values in ascending order: \", index.numpy())\n",
        "\n",
        "# Descending Orders\n",
        "values_desc, index_desc = tf.nn.top_k(sort_sample_1, 5, sorted=False)\n",
        "print(\"K smallest values in descending order: \", values_desc.numpy())\n",
        "print(\"Indices of the smallest values in descending order: \", index_desc.numpy())"
      ],
      "metadata": {
        "id": "CAbnIfk7ABpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Eager Execution Mode of TensorFlow 2**\n",
        "\n",
        "*Eager execution mode:*\n",
        "\n",
        "The eager execution mode of TensorFlow is a type of imperative programming, which is the same as the native Python. When you perform a particular operation, the system immediately returns a result.\n",
        "\n",
        "*Graph mode:*\n",
        "\n",
        "TensorFlow 1 adopts the graph mode to first build a computational graph, enable a session, and then feed actual data to obtain a result.\n",
        "In eager execution mode, code debugging is easier, but the code execution efficiency is lower.\n",
        "The following implements simple multiplication by using TensorFlow to compare the differences between the eager execution mode and the graph mode.\n",
        ""
      ],
      "metadata": {
        "id": "JWNnz1KXKOG7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x=tf.ones((2,2), dtype=tf.dtypes.float32)\n",
        "y=tf.constant([[1,2],[3,4]], dtype=tf.dtypes.float32)\n",
        "z=tf.matmul(x,y)\n",
        "print(z)"
      ],
      "metadata": {
        "id": "Te_UFHsLLYoq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# use the syntax of TensorFlow 1.x in TensorFlow 2.x. You can install the v1 compatibility package in TensorFlow 2 to inherit the TensorFlow 1.x code and disable the eager execution mode.\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_eager_execution()  # Optional (already disabled in v1 compatibility mode)\n",
        "\n",
        "# Create tensors and define the computation graph\n",
        "a = tf.ones((2, 2), dtype=tf.float32)  # Simplified dtype\n",
        "b = tf.constant([[1, 2], [3, 4]], dtype=tf.float32)\n",
        "c = tf.matmul(a, b)  # Matrix multiplication\n",
        "\n",
        "# Run the graph in a session\n",
        "with tf.Session() as sess:  # Fixed: tf.Session() instead of tf.session()\n",
        "    result = sess.run(c)\n",
        "    print(result)"
      ],
      "metadata": {
        "id": "l4L82eZvL8Ia"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**AutoGraph of TensorFlow 2**\n",
        "\n",
        "When uses to comment out a function, the tf.function decorator can be called like any other function. tf.function will be compiled into a graph, so that it can run more efficiently on a GPU or TPU. In this case, the function becomes an operation in TensorFlow. The function can be directly called to output a return value. However, the function is executed in graph mode and the immediate variable values cannot be directly viewed."
      ],
      "metadata": {
        "id": "kHJs07TBNa45"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def simple_nn_layer(w,x,b):\n",
        "    print(b)\n",
        "    return tf.nn.relu(tf.matmul(w,x)+b)\n",
        "\n",
        "w=tf.random.uniform((3,3))\n",
        "x=tf.random.uniform((3,3))\n",
        "b=tf.constant(0.5)\n",
        "simple_nn_layer(w,x,b)\n"
      ],
      "metadata": {
        "id": "OLwGDYzrORnR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "According to the output result, the value of b in the function cannot be directly viewed, but the return value can be viewed using  .numpy().\n",
        "The following compares the performance of the graph and eager execution modes by performing the same operation (LSTM computation of one layer)\n"
      ],
      "metadata": {
        "id": "bP-tkac1PShS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# use the timeit module to measure the execution time of a small code segment\n",
        "\n",
        "import timeit\n",
        "\n",
        "# Create a convolutional layer\n",
        "CNN_cell = tf.keras.layers.Conv2D(filters=100, kernel_size=2, strides=(1,1))\n",
        "\n",
        "# Use @ tf.function to convert the operation into a graph\n",
        "@tf.function\n",
        "def CNN_fn(image):\n",
        "    return CNN_cell(image)\n",
        "\n",
        "\n",
        "image = tf.zeros([100, 200, 200, 3])\n",
        "\n",
        "# Compare the execution time of the two modes\n",
        "\n",
        "CNN_cell(image)\n",
        "CNN_fn(image)\n",
        "\n",
        "# Call timeit.timeit to measure the time required for executing the code 10 time\n",
        "print(\"Time required for performing the computation of one convulutional neural network (CNN) layer in eager execution mode: \", timeit.timeit(lambda: CNN_cell(image), number=10))\n",
        "print(\"Time Required for performing the computation of one CNN layer in graph mode: \", timeit.timeit(lambda: CNN_fn(image), number=10))\n",
        "\n"
      ],
      "metadata": {
        "id": "kCNXiDY0PTXR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***NOTE:***\n",
        "The comparison shows that the code execution efficiency in graph mode is much higher. Therefore, the @tf.function can be used to improve the code execution efficiency."
      ],
      "metadata": {
        "id": "wFnwdGoyRSOi"
      }
    }
  ]
}