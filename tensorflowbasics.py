# -*- coding: utf-8 -*-
"""TensorFlowBasics.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/151ICeR2mz3IUdu9oqGAtpQcVML0HoFg7

### **TensorFlow Basics (HCIA-AI)**

**Introduction to Tensors**

In TensorFlow, tensors are classified into constant and variable tensors:
* A defined constant tensor has an immutable value and dimension while a defined variable tensor has a variable value and an immutable dimension.
* In a neural network, a variable tensor is generally used as a matrix for storing weights and other information, and is trainable data type. A constant tensor can be used as a variable for storing hyperparameters or other structural information.

**Creating a Constant Tensor**

Methods for creating tensors:

* tf.constant() - create a constant tensor
* tf.zeros(), tf.zeros_like(), tf.ones(), tf.ones_like() creates an all-zero or all-one constant tensors.
* tf.fill() creates a tensor with user-defined value
* tf.random() creates a tensor with a known distribution
* tf.convert_to_tensor() creates a list object by using NumPy and then converts it into a tensor

tf.constant(value,dtype=None,shape=None,name='Const', verify_shape=False);
* value:value
* dtype:data type
* shape:tensor shape
* name: name for the constant tensor
* verify_shape : Boolean that enables verification of a shape of values. The default value is False. If verify_shape is set to True, the system checks weather the shape of value is consistent with shape. If they are inconsistent, an error is reported.

**# Step1 - tf.constant()**
"""

# Step1 - tf.constant()
import tensorflow as tf
print(tf.__version__)

# Creating 2x2 matrix with values 1, 2,3,4
const_a = tf.constant([1,2,3,4],shape=[2,2],dtype=tf.float32)

print(const_a)

# view common attributes
print("Value of const_a",const_a.numpy())
print("Data type of const_a",const_a.dtype)
print("Shape of const_a",const_a.shape)
print("device that const_a will be generated:",const_a.device)

"""**Step 2 tf.zeros(), tf.zeros_like(), tf.ones(), tf.ones_like()**"""

# create an all-zero tensor
tensor_zeros = tf.zeros(shape=(3, 3), dtype=tf.float32)
print("tf.zeros():\n", tensor_zeros.numpy())

# create an all-zero tensor with the same shape and dtype as another tensor
tensor_zeros_like = tf.zeros_like(const_a)
print("tf.zeros_like(const_a):\n", tensor_zeros_like.numpy())

# create an all-one tensor
tensor_ones = tf.ones(shape=(2, 4), dtype=tf.int32)
print("tf.ones():\n", tensor_ones.numpy())

# create an all-one tensor with the same shape and dtype as another tensor
tensor_ones_like = tf.ones_like(const_a)
print("tf.ones_like(const_a):\n", tensor_ones_like.numpy())

"""**Step 3 tf.fill()**"""

# create a tensor with user-defined value
fill_d = tf.fill(dims=(2, 3), value=8)
print("tf.fill():\n", fill_d.numpy())

"""**Step 4 tf.random**

tf.random is used to generate a tensor with a specific distribution. The common methods includes tf.random.uniform(), tf.random.normal() and tf.random.shuffle().
"""

# Step 4 tf.random

# tf.random.normal: Generates a tensor with random values drawn from a normal distribution.
random_normal = tf.random.normal(shape=(2, 2), mean=0.0, stddev=1.0, dtype=tf.float32)
print("tf.random.normal():\n", random_normal.numpy())

# tf.random.uniform: Generates a tensor with random values drawn from a uniform distribution.
random_uniform = tf.random.uniform(shape=(3, 3), minval=0, maxval=10, dtype=tf.int32)
print("tf.random.uniform():\n", random_uniform.numpy())

# tf.random.uniform: Generates a tensor with random values drawn from a uniform distribution (float).
random_uniform_float = tf.random.uniform(shape=(2, 3), minval=0.0, maxval=1.0, dtype=tf.float32)
print("tf.random.uniform(float):\n", random_uniform_float.numpy())

# tf.random.truncated_normal: Generates a tensor with random values drawn from a truncated normal distribution.
random_truncated_normal = tf.random.truncated_normal(shape=(4, 4), mean=0.0, stddev=1.0, dtype=tf.float32)
print("tf.random.truncated_normal():\n", random_truncated_normal.numpy())

# tf.random.shuffle: Randomly shuffles a tensor along its first dimension.
tensor_to_shuffle = tf.constant([[1, 2], [3, 4], [5, 6]], dtype=tf.float32)
shuffled_tensor = tf.random.shuffle(tensor_to_shuffle)
print("tf.random.shuffle():\n", shuffled_tensor.numpy())

# tf.random.stateless_normal: Generates a tensor with random values from a normal distribution given a seed.
# Requires a seed tensor.
seed = tf.constant([123, 456], dtype=tf.int32)
stateless_normal = tf.random.stateless_normal(shape=(2, 2), seed=seed)
print("tf.random.stateless_normal():\n", stateless_normal.numpy())

# tf.random.stateless_uniform: Generates a tensor with random values from a uniform distribution given a seed.
stateless_uniform = tf.random.stateless_uniform(shape=(3, 3), seed=seed, minval=0, maxval=10)
print("tf.random.stateless_uniform():\n", stateless_uniform.numpy())

# tf.random.stateless_truncated_normal: Generates a tensor with random values from a truncated normal distribution given a seed.
stateless_truncated_normal = tf.random.stateless_truncated_normal(shape=(4, 4), seed=seed)
print("tf.random.stateless_truncated_normal():\n", stateless_truncated_normal.numpy())

# tf.random.stateless_shuffle: Randomly shuffles a tensor along its first dimension given a seed.
# stateless_shuffled = tf.random.stateless_shuffle(tensor_to_shuffle, seed=seed)
stateless_shuffled = tf.random.experimental.stateless_shuffle(tensor_to_shuffle, seed=seed)
print("tf.random.stateless_shuffle():\n", stateless_shuffled.numpy())

# tf.random.categorical: Draws samples from a categorical distribution.
logits = tf.constant([[2.0, 1.0, 0.1], [0.5, 1.5, 1.0]])
num_samples = 5
categorical_samples = tf.random.categorical(logits, num_samples)
print("tf.random.categorical():\n", categorical_samples.numpy())

# tf.random.stateless_categorical: Draws samples from a categorical distribution given a seed.
stateless_categorical_samples = tf.random.stateless_categorical(logits, num_samples, seed=seed)
print("tf.random.stateless_categorical():\n", stateless_categorical_samples.numpy())

# Setting a global seed for reproducible results across different calls
tf.random.set_seed(42)
print("\nAfter setting global seed 42:")
random_normal_seeded = tf.random.normal(shape=(2, 2))
print("tf.random.normal() with global seed:\n", random_normal_seeded.numpy())

# Resetting the global seed to observe different results
tf.random.set_seed(None)
print("\nAfter resetting global seed:")
random_normal_unseeded = tf.random.normal(shape=(2, 2))
print("tf.random.normal() after resetting global seed:\n", random_normal_unseeded.numpy())

"""**Step 5 Create a list object by using NumPy and then convert it into a tensor by using tf.convert_to_tensor.**

This method can connvert the given value to a tensor. It converts Python objects of various types to Tensor objects

tf.convert_to_tensor(value,dtype=None,dtpe_hint=None, name=None):

* value : value to be converted
* dtype : tensor data type
* dtype_hint: optional element type for the returned tensor, used when dtype is None. In some cases, a caller may not have a dtype in mind when converting to a tensor, so dtype_int can be used as a soft preference.

**Why tf.convert_to_tensor**

The tf.convert_to_tensor function in TensorFlow is used to convert a given input (e.g., a Python list, NumPy array, or scalar) into a TensorFlow tensor. This is essential for integrating data with TensorFlow's computational graph and enabling operations like automatic differentiation, GPU acceleration, and model training.
"""

# Step 5 tf.convert_to_tensor

# create a list
list_f = [1,2,3,4,5,6]

# view the data type
type(list_f)

tensor_f = tf.convert_to_tensor(list_f,dtype=tf.float32)
print(tensor_f)

"""**Creating a Variable Tensor**

In TensorFlow, variables are created and tracked via the tf.Variable class. A tf.Variable represents a tensor whose value can be changed by running ops on it. Specific ops allow you to read and modify the values.
"""

# To create a variable, provide an initial value

var_1 = tf.Variable(tf.ones([2,3]))
var_1

# Read the variable value
print("value of var_1:", var_1.read_value())

# assign a variable value
var_value_1 = [[1,2,3],[4,5,6]]
var_1.assign(var_value_1)

# print the new value
print("new value of var_1:", var_1.read_value())

# add value to this variable
var_1.assign_add(tf.ones([2,3]))
print("new value of var_1:", var_1.read_value())

var_1

"""**Tensor Slicing**

* slicing is used to create mini-batches for training neural networks, isolate specific channels in image data, or extract time steps in sequence models like RNNs.

* slicing provides flexibility in handling complex, high-dimensional data (e.g., images, videos, or time-series), making it easier to implement algorithms that require specific data views or transformations. By avoiding unnecessary data copying and enabling precise access to tensor elements, slicing enhances memory efficiency and computational speed, which are crucial for large-scale machine learning tasks

Major slicing methods include:
* [start:end] : extract a data slice from the start position to the end position of a tensor
* [start:end:step] or [::step] : extracts a data slice at an internal of step from the start position to the end position of a tensor
* [::-1] : slice data from the last element
* '...': indicates a data slice of any length

"""

# Create a 4 dimensional tensor. The tensor contains four images. the size of each image is 100 x 100 x 3

tensor_h = tf.random.normal([4,100,100,3])
tensor_h

# Extract the first image

tensor_h[0,:,:,:]

# extract one slice every two images

tensor_h[::2,...]

# Slice data from last element

tensor_h[::-1]

"""**Indexing**

The basic format of an index is a[d1][d2][d3]

If the indexes to be extracted are nonconsecutive, tf.gather and tf.gather_nd are commonly used for data extraction in TensorFlow
"""

# Obtain the pixel in the [20,40] position in the second channel of the first image

tensor_h[0][19][39][1]

"""**To extract data from a particular dimension:**

*tf.gather(params, indices, axis=None):*

* *params* :input tensor
* *indices* : index of the data to be extracted
* *axis* : dimension of the data to be extracted
"""

# Extract the first, second, and fourth images from tensor_h([4,100,100,3])

indices = [0,1,3]

tf.gather(tensor_h,indices=indices,axis=0)

"""tf.gather_nd allows data extraction from multiple dimensions:

* params : input tensor
* indices : index of the data to be extracted. generally, this is a multidimensional list.
"""

# Extract the pixel in [1,1] in the first dimension of the first image and pixel in [2,2] in the first dimension of the second image in tensor_h([4,100,100,3])

indices = [[0,1,1,0],[1,2,2,0]]

tf.gather_nd(tensor_h,indices=indices)

"""**Dimension Display**

Note:
* .shape and .get_shape() return TensorShape objects, while tf.shape(x) return Tensor objects
"""

const_d_1 = tf.constant([[1,2,3,4]],shape=[2,2],dtype=tf.float32)
print(const_d_1)

print(const_d_1.shape)
print(const_d_1.get_shape())

# The output is a tensor. The value of the tensor indicates the size of the tensor dimension to be displayed
print(tf.shape(const_d_1))

"""Dimension Reshaping

tf.reshape(tensor, shape, name=None):
* tensor: input tensor
* shape : shape of the reshaped tensor
"""

reshape_1 = tf.constant([[1,2,3],[4,5,6]])

print(reshape_1)

print(tf.reshape(reshape_1,shape=[3,2]))

"""**Dimnsion Expansion**

tf.expand_dims(input, axix,name=None)
* input : input tensor
* axix : adds a dimension after the axis dimension. Given an input of D dimensions, axis must be in range[-(d+1),D] (inclusive). A negative value indicates the reverse order.
"""

# Generate a 100x100x3 tensor to reppresent a 100x100 three-channel color image.

expand_sample_1 = tf.random.normal([100,100,3],seed=1)

print("origional data size : ", expand_sample_1.shape)

print("add a dimension (axix=0) before the first dimension : ", tf.expand_dims(expand_sample_1, axis=0).shape)

print("add a dimension (axix=1) before the second dimension : ", tf.expand_dims(expand_sample_1, axis=1).shape)

print("add a dimension (axix=-1) before the last dimension : ", tf.expand_dims(expand_sample_1, axis=-1).shape)

"""**Dimension Squeezing**

tf.squeeze(input, axis=None, name=None)

This method is used to remove dismensions of size 1 from the shape of a tensor.
* input : input tensor
* axis: if you do not want to remove all size dimensions, remove specific size 1 dimensions by specifying axis
"""

# Generate a 100x100x3 tensor

orig_ample_1 = tf.random.normal([1,100,100,3])

print("origional data size : ", orig_ample_1.shape)

sqeezed_sample_1 = tf.squeeze(orig_ample_1)

print("squeezed data size : ", sqeezed_sample_1.shape)

# The dimension of sqeeze_sampme_2 is [1,2,1,3,1,1]
sqeezed_sample_2 = tf.random.normal([1,2,1,3,1,1])
t_1=tf.squeeze(sqeezed_sample_2)
print("t_1.shape : ",t_1.shape)

# Remove a specific dimenion
# t is a tensor of shape [1,2,1,3,1,1]

t_1_new = tf.squeeze(sqeezed_sample_2,[2,2])
print("t_1_new.shape : ",t_1_new.shape)

"""**Transpose**

tf.transpose(a,perm=None, conjugate=False,name='transpose')
* a: input tensor
* perm: permutation of the dimensions of a, generally used to transpose high-dimensional arrays
* conjugate: conjugate tranpose
* name: name of the operation

"""

# Low-dimensional transposition in simple. Input the tensor to be transposed by calling tf.transpose

transpose_sample_1 = tf.constant([1,2,3,4,5,6],shape=[2,3])
print("Origional data size", transpose_sample_1.shape)

transposed_sample_1 = tf.transpose(transpose_sample_1)
print("Tranposed data size",transposed_sample_1.shape)

"""The perm parameter is required for transposing high -dimensional data. perm indicates the permutation of the dimensions of the input tensor.
For a three-dimensional tensor, its origional dimension permutation is [0,1,2] (perm), including the length, width, and height of the high-dimensional data, respectively.
By changing the value  squeeze in per, we can transpose the corresponnding dimension of the data.
"""

# Generate a 4x100x200x3 tensor to represent four 100x200 three-channel color images

trans_sample_2 = tf.random.normal([4,100,200,3])
print("Origional data size : ", trans_sample_2.shape)

# Exchange the length and width of the four images. The value of perm is changed from [0,1,2,3] to [0,2,1,3]

transposed_sample_2 = tf.transpose(trans_sample_2,perm=[0,2,1,3])
print("Transposed data size : ", transposed_sample_2.shape)
#

"""**Broadcast (broadcast_to)**

broadcast_to is used to broadcast data from a low dimension to a high dimension.
*tf.broadcast_to(input,shape,name=None)*

* input : input tensor
* shape : size of the output tensor
"""

broadcast_sample_1 = tf.constant([1,2,3,4,5,6])
print("Origional data size : ", broadcast_sample_1.numpy())

broadcasted_sample_1 = tf.broadcast_to(broadcast_sample_1,shape=[4,6])
print("Broadcasted data size : ", broadcasted_sample_1.numpy())

# During the operation, if two arrays have different shapes, TensorFlow automatically triggers the broadcast mechanism as NumPy does

a=tf.constant([[0,0,0],[10,10,10],[20,20,20],[30,30,30]])
b=tf.constant([1,2,3])

print(a+b)

"""# **Arithmatic Operations of Tensors**

**Arithmatics Operators**

Arithmatic operations include addition(tf.add), subtraction(tf.subtract), multiplication(tf.multiply), division(tf.divide), logarithm(tf.math.log), and powers(tf.pow).
"""

# Addition
tensor_a = tf.constant([1, 2, 3], dtype=tf.float32)
tensor_b = tf.constant([4, 5, 6], dtype=tf.float32)
addition_result = tf.add(tensor_a, tensor_b)
print("Addition:", addition_result.numpy())

# Subtraction
subtraction_result = tf.subtract(tensor_b, tensor_a)
print("Subtraction:", subtraction_result.numpy())

# Multiplication (element-wise)
multiplication_result = tf.multiply(tensor_a, tensor_b)
print("Multiplication:", multiplication_result.numpy())

# Division (element-wise)
tensor_c = tf.constant([10.0, 20.0, 30.0], dtype=tf.float32)
tensor_d = tf.constant([2.0, 5.0, 10.0], dtype=tf.float32)
division_result = tf.divide(tensor_c, tensor_d)
print("Division:", division_result.numpy())

# Powers
tensor_f = tf.constant([2.0, 3.0, 4.0], dtype=tf.float32)
tensor_g = tf.constant([2.0, 3.0, 0.5], dtype=tf.float32)
powers_result = tf.pow(tensor_f, tensor_g)
print("Powers:", powers_result.numpy())

# Logarithm of a tensor
positive_tensor = tf.constant([[1.0, 2.0], [3.0, 4.0]], dtype=tf.float32)
log_positive_tensor = tf.math.log(positive_tensor)
print("Logarithm of a tensor with positive elements:\n", log_positive_tensor.numpy())

"""**Matrix Multiplication**"""

# Matrix Multiplication

# 2D Tensors
tensor_2d_a = tf.constant([[1, 2], [3, 4]], dtype=tf.float32)
tensor_2d_b = tf.constant([[5, 6], [7, 8]], dtype=tf.float32)
matmul_2d = tf.matmul(tensor_2d_a, tensor_2d_b)
print("Matrix multiplication of 2D tensors:\n", matmul_2d.numpy())

# 3D Tensors
# Shape: (batch_size, rows, columns)
tensor_3d_a = tf.constant([[[1, 2], [3, 4]], [[5, 6], [7, 8]]], dtype=tf.float32) # Shape (2, 2, 2)
tensor_3d_b = tf.constant([[[9, 10], [11, 12]], [[13, 14], [15, 16]]], dtype=tf.float32) # Shape (2, 2, 2)
matmul_3d = tf.matmul(tensor_3d_a, tensor_3d_b)
print("Matrix multiplication of 3D tensors:\n", matmul_3d.numpy())

# 4D Tensors
# Shape: (batch_size, channels, rows, columns) or (batch_size, height, width, channels) - depends on convention
# Let's use (batch_size, channels, rows, columns)
tensor_4d_a = tf.constant([[[[1, 2], [3, 4]], [[5, 6], [7, 8]]]], dtype=tf.float32) # Shape (1, 2, 2, 2)
tensor_4d_b = tf.constant([[[[9, 10], [11, 12]], [[13, 14], [15, 16]]]], dtype=tf.float32) # Shape (1, 2, 2, 2)
matmul_4d = tf.matmul(tensor_4d_a, tensor_4d_b)
print("Matrix multiplication of 4D tensors:\n", matmul_4d.numpy())

"""**Tensor Statistics Collection**

Methods for collecting tensor statistics include:
* tf.reduce_min/max/mean() : calculates minimum, maximum, and mean values.
* tf.argmax()/tf.argmin(): calculates the position of the maximum and minimum values
* tf.equal(): check weather two tensors are equal by element
* tf.unique(): removes duplicate elements from a tensor
* tf.nn.in_top_k(prediction, target, K): calculates weather the predicted value is equal to the actual value and returns a tensor of the Boolean type



"""

# tf.reduce_min/max/mean
tensor_stats = tf.constant([[1, 2, 3], [4, 5, 6]], dtype=tf.float32)
min_value = tf.reduce_min(tensor_stats)
max_value = tf.reduce_max(tensor_stats)
mean_value = tf.reduce_mean(tensor_stats)
print("Tensor for statistics:", tensor_stats.numpy())
print("tf.reduce_min:", min_value.numpy())
print("tf.reduce_max:", max_value.numpy())
print("tf.reduce_mean:", mean_value.numpy())

# You can also reduce along specific axes
min_along_axis_0 = tf.reduce_min(tensor_stats, axis=0)
max_along_axis_1 = tf.reduce_max(tensor_stats, axis=1)
print("tf.reduce_min along axis 0:", min_along_axis_0.numpy())
print("tf.reduce_max along axis 1:", max_along_axis_1.numpy())

# tf.argmax()/tf.argmin()
tensor_arg = tf.constant([[1, 5, 3], [4, 2, 6]], dtype=tf.float32)
argmax_value = tf.argmax(tensor_arg) # Default axis is the last axis
argmin_value = tf.argmin(tensor_arg) # Default axis is the last axis
print("Tensor for argmax/argmin:", tensor_arg.numpy())
print("tf.argmax (along last axis):", argmax_value.numpy())
print("tf.argmin (along last axis):", argmin_value.numpy())

# Argmax/argmin along specific axes
argmax_axis_0 = tf.argmax(tensor_arg, axis=0)
argmin_axis_1 = tf.argmin(tensor_arg, axis=1)
print("tf.argmax along axis 0:", argmax_axis_0.numpy())
print("tf.argmin along axis 1:", argmin_axis_1.numpy())

# tf.equal()
tensor_eq1 = tf.constant([[1, 2], [3, 4]])
tensor_eq2 = tf.constant([[1, 5], [3, 4]])
equal_result = tf.equal(tensor_eq1, tensor_eq2)
print("Tensor 1 for equality:", tensor_eq1.numpy())
print("Tensor 2 for equality:", tensor_eq2.numpy())
print("tf.equal:", equal_result.numpy())

# tf.unique()
tensor_unique = tf.constant([1, 5, 2, 5, 3, 1, 4])
unique_elements, original_indices = tf.unique(tensor_unique)
print("Tensor for unique:", tensor_unique.numpy())
print("tf.unique elements:", unique_elements.numpy())
print("tf.unique original indices:", original_indices.numpy())

# For a 2D tensor, tf.unique works on the flattened tensor
tensor_unique_2d = tf.constant([[1, 5], [2, 5], [3, 1], [4, 4]])
unique_elements_2d, original_indices_2d = tf.unique(tf.reshape(tensor_unique_2d, [-1]))
print("Tensor for unique (2D, flattened):", tensor_unique_2d.numpy())
print("tf.unique elements (flattened):", unique_elements_2d.numpy())
print("tf.unique original indices (flattened):", original_indices_2d.numpy())


# tf.nn.in_top_k(predictions, targets, k)
# This function is typically used in classification tasks to check if the true target is among the top K predictions.
# Note: tf.nn.in_top_k is deprecated in TensorFlow 2.0+. Use tf.math.in_top_k instead.

# Example with tf.math.in_top_k
predictions = tf.constant([[0.1, 0.9, 0.2], [0.8, 0.2, 0.0]], dtype=tf.float32) # Example probabilities or logits for 2 examples and 3 classes
targets = tf.constant([1, 0], dtype=tf.int32) # True class indices for the 2 examples
k_value = 1 # Check if the top 1 prediction is correct
print(k_value)

"""Dimensions

**Dimensions-based Arithmetic Operations**

In TensorFlow, operations such as tf.reduce_* tensor dimensions. These operations can be performed on the dimension elements of a tensor, for example, calculating the mean value by row and calculating a product of all elements in the tensor.
Common operations include tf.reduce_sum(addition), tf.reduce_prod(multiplication), tf.reduce_min(minimum), tf.reduce_max(maximum), tf.reduce_mean(mean), tf.reduce_all(logical AND), tf.reduce_any(logical OR), tf.reduce_logsumexp(log(sum(exp))).
"""

# Create a simple tensor
tensor_reduce = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]], dtype=tf.float32)
print("Original tensor:\n", tensor_reduce.numpy())

# tf.reduce_sum (addition)
sum_all = tf.reduce_sum(tensor_reduce)
print("\ntf.reduce_sum (all elements):", sum_all.numpy())
sum_axis_0 = tf.reduce_sum(tensor_reduce, axis=0) # Sum along columns
print("tf.reduce_sum (axis=0):", sum_axis_0.numpy())
sum_axis_1 = tf.reduce_sum(tensor_reduce, axis=1) # Sum along rows
print("tf.reduce_sum (axis=1):", sum_axis_1.numpy())

# tf.reduce_prod (multiplication)
prod_all = tf.reduce_prod(tensor_reduce)
print("\ntf.reduce_prod (all elements):", prod_all.numpy())
prod_axis_0 = tf.reduce_prod(tensor_reduce, axis=0) # Product along columns
print("tf.reduce_prod (axis=0):", prod_axis_0.numpy())
prod_axis_1 = tf.reduce_prod(tensor_reduce, axis=1) # Product along rows
print("tf.reduce_prod (axis=1):", prod_axis_1.numpy())

# tf.reduce_min (minimum)
min_all = tf.reduce_min(tensor_reduce)
print("\ntf.reduce_min (all elements):", min_all.numpy())
min_axis_0 = tf.reduce_min(tensor_reduce, axis=0) # Minimum along columns
print("tf.reduce_min (axis=0):", min_axis_0.numpy())
min_axis_1 = tf.reduce_min(tensor_reduce, axis=1) # Minimum along rows
print("tf.reduce_min (axis=1):", min_axis_1.numpy())

# tf.reduce_max (maximum)
max_all = tf.reduce_max(tensor_reduce)
print("\ntf.reduce_max (all elements):", max_all.numpy())
max_axis_0 = tf.reduce_max(tensor_reduce, axis=0) # Maximum along columns
print("tf.reduce_max (axis=0):", max_axis_0.numpy())
max_axis_1 = tf.reduce_max(tensor_reduce, axis=1) # Maximum along rows
print("tf.reduce_max (axis=1):", max_axis_1.numpy())

# tf.reduce_mean (mean)
mean_all = tf.reduce_mean(tensor_reduce)
print("\ntf.reduce_mean (all elements):", mean_all.numpy())
mean_axis_0 = tf.reduce_mean(tensor_reduce, axis=0) # Mean along columns
print("tf.reduce_mean (axis=0):", mean_axis_0.numpy())
mean_axis_1 = tf.reduce_mean(tensor_reduce, axis=1) # Mean along rows
print("tf.reduce_mean (axis=1):", mean_axis_1.numpy())

# tf.reduce_all (logical AND) - requires boolean tensor
tensor_bool = tf.constant([[True, False, True], [True, True, False]], dtype=tf.bool)
print("\nOriginal boolean tensor:\n", tensor_bool.numpy())
all_all = tf.reduce_all(tensor_bool)
print("tf.reduce_all (all elements):", all_all.numpy())
all_axis_0 = tf.reduce_all(tensor_bool, axis=0) # AND along columns
print("tf.reduce_all (axis=0):", all_axis_0.numpy())
all_axis_1 = tf.reduce_all(tensor_bool, axis=1) # AND along rows
print("tf.reduce_all (axis=1):", all_axis_1.numpy())

# tf.reduce_any (logical OR) - requires boolean tensor
any_all = tf.reduce_any(tensor_bool)
print("\ntf.reduce_any (all elements):", any_all.numpy())
any_axis_0 = tf.reduce_any(tensor_bool, axis=0) # OR along columns
print("tf.reduce_any (axis=0):", any_axis_0.numpy())
any_axis_1 = tf.reduce_any(tensor_bool, axis=1) # OR along rows
print("tf.reduce_any (axis=1):", any_axis_1.numpy())

# tf.reduce_logsumexp (log(sum(exp)))
# This is often used in the log-probability calculation, e.g., for softmax.
tensor_logsumexp = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]], dtype=tf.float32)
print("\nOriginal tensor for logsumexp:\n", tensor_logsumexp.numpy())
logsumexp_all = tf.reduce_logsumexp(tensor_logsumexp)
print("tf.reduce_logsumexp (all elements):", logsumexp_all.numpy())
logsumexp_axis_0 = tf.reduce_logsumexp(tensor_logsumexp, axis=0) # LogSumExp along columns
print("tf.reduce_logsumexp (axis=0):", logsumexp_axis_0.numpy())
logsumexp_axis_1 = tf.reduce_logsumexp(tensor_logsumexp, axis=1) # LogSumExp along rows
print("tf.reduce_logsumexp (axis=1):", logsumexp_axis_1.numpy())

"""**Tensor Concatenation**

In TensorFlow, tensor concatenation operations include:
* tf.concat(): concatenates tensors along one dimesnions remain unchanged
* tf.stack():stacks the tensor list of rank R into a tensor of rank (R+1). Dimensins are changed after stacking.

*tf.concat(values, axis, name='concat')*

* values: input tensor
* axis: dimension along which to concatenate
* name: name for operation

"""

concat_sample_1=tf.random.normal([4,100,100,3])
concat_sample_2=tf.random.normal([40,100,100,3])

# Print origional tensor
print("Origional data size", concat_sample_1.shape,concat_sample_2.shape)

concat_sample_1 = tf.concat([concat_sample_1,concat_sample_2],axis=0)

print("Concatenated data size", concat_sample_1.shape)

"""A dimension is added to an origional matrix in the same way. axis determines the position where the dimension is added.

*tf.stack(values, axis=0, name='stack')*
* values: a list of tensor objects with the same shape and type
* axis: axis to stack along
* name: name for the operation
"""

stack_sample_1 = tf.random.normal([4, 100, 100, 3])
stack_sample_3 = tf.random.normal([4, 100, 100, 3])  # Shape matches stack_sample_1

# Print original tensor shapes
print("Original data size:", stack_sample_1.shape, stack_sample_3.shape)

# Stack along axis=0 (adds a new dimension at the beginning)
stacked_sample = tf.stack([stack_sample_1, stack_sample_3], axis=0)  # Fixed variable name

print("Stacked data size:", stacked_sample.shape)

"""**Tensor Splitting**

In TensorFlow, tensor splitting operations include:
* tf.unstack(): unpacks tensors along the specific dimension
* tf.split(): splits a tensor into a list of sub tensors based on specific dimensions

Compared with tf.unstack(), tf.split() is more flexible.

*tf.unstack(value,num=None, axis=0, name='unstack')*

* value: input tensor
* num: outputs a list containing num elements. num must be equal to the number of elements in the specified dimension. Generally, this parameter is ignored.
* axis: axis to unstack along
* name: name for the operation


"""

# unpack data along the first dimension and output data in a list
tf.unstack(stacked_sample,axis=0)

"""*tf.split(value,num_or_size_splits, axis=0)*

* value: input tensor
* num_or_size_splits: number of splits
* axis: dimension along which to split

tf.split() can be split in either of the following ways:
* If num_or_size_splits is an integer, the tensor is evenly split into several small tensors along the axis=D
* if num_or_size_splits is a vector, the tensor is split into several smaller tensors based on the element values of the vector along the axis=D dimension

"""

import numpy as np

split_sample_1=tf.random.normal([10,100,100,3])

print("Origional data size", split_sample_1.shape)

splited_sample_1 = tf.split(split_sample_1, num_or_size_splits=5, axis=0)

print("If num_or_size_splits=5 the size of Splited data is: ", np.shape(splited_sample_1))

splited_sample_2 = tf.split(split_sample_1, num_or_size_splits=[3,5,2], axis=0)

print("If num_or_size_splits=[3,5,2] the size of Splited data is: ", np.shape(splited_sample_2[0]), np.shape(splited_sample_2[1]), np.shape(splited_sample_2[2]))

"""**Tensor Sorting**

In TensorFlow, tensor sorting operations include:
* tf.sort(): sorts tensors in ascending or descending order and returns the sorted tensors.
* tf.argsort(): sorts tensors in ascending or descending order and returns indices
* tf.nn.top_k(): returns the k largest values

*tf.sort/argsort(input,direction, axis):*

* input: input tensor
* direction: direction in which to sort the values. The value can be ascending or descending. The default value is ascending.
* axis: axis along which to sort the default value is -1, which sorts the last axis.
"""

sort_sample_1 = tf.random.shuffle(tf.range(10))

print("Input tensor", sort_sample_1.numpy())

sorted_sample_1 = tf.sort(sort_sample_1, direction="ASCENDING")
print("Ascending order", sorted_sample_1.numpy())

sorted_sample_2 = tf.sort(sort_sample_1, direction="DESCENDING")
print("Descending order", sorted_sample_2.numpy())

"""*tf.nn.top_k(input, K, sorted=TRUE):*

* input: input tensor
* K : k largest values to be output and their indices
* sorted: sorted=TRUE indicates in ascending order. sorted=FALSE indicates in descending order.

Two tensors are returned:
* values: k largest values in each row
* indices: indices of values within the last dimension of input
"""

# Ascending order
values, index = tf.nn.top_k(sort_sample_1, 5)
print("Input tensor", sort_sample_1.numpy())
print("K largest values in ascending order: ", values.numpy())
print("Indices of the largest values in ascending order: ", index.numpy())

# Descending Orders
values_desc, index_desc = tf.nn.top_k(sort_sample_1, 5, sorted=False)
print("K smallest values in descending order: ", values_desc.numpy())
print("Indices of the smallest values in descending order: ", index_desc.numpy())

"""**Eager Execution Mode of TensorFlow 2**

*Eager execution mode:*

The eager execution mode of TensorFlow is a type of imperative programming, which is the same as the native Python. When you perform a particular operation, the system immediately returns a result.

*Graph mode:*

TensorFlow 1 adopts the graph mode to first build a computational graph, enable a session, and then feed actual data to obtain a result.
In eager execution mode, code debugging is easier, but the code execution efficiency is lower.
The following implements simple multiplication by using TensorFlow to compare the differences between the eager execution mode and the graph mode.

"""

x=tf.ones((2,2), dtype=tf.dtypes.float32)
y=tf.constant([[1,2],[3,4]], dtype=tf.dtypes.float32)
z=tf.matmul(x,y)
print(z)

# use the syntax of TensorFlow 1.x in TensorFlow 2.x. You can install the v1 compatibility package in TensorFlow 2 to inherit the TensorFlow 1.x code and disable the eager execution mode.
import tensorflow.compat.v1 as tf
tf.disable_eager_execution()  # Optional (already disabled in v1 compatibility mode)

# Create tensors and define the computation graph
a = tf.ones((2, 2), dtype=tf.float32)  # Simplified dtype
b = tf.constant([[1, 2], [3, 4]], dtype=tf.float32)
c = tf.matmul(a, b)  # Matrix multiplication

# Run the graph in a session
with tf.Session() as sess:  # Fixed: tf.Session() instead of tf.session()
    result = sess.run(c)
    print(result)

"""**AutoGraph of TensorFlow 2**

When uses to comment out a function, the tf.function decorator can be called like any other function. tf.function will be compiled into a graph, so that it can run more efficiently on a GPU or TPU. In this case, the function becomes an operation in TensorFlow. The function can be directly called to output a return value. However, the function is executed in graph mode and the immediate variable values cannot be directly viewed.
"""

@tf.function
def simple_nn_layer(w,x,b):
    print(b)
    return tf.nn.relu(tf.matmul(w,x)+b)

w=tf.random.uniform((3,3))
x=tf.random.uniform((3,3))
b=tf.constant(0.5)
simple_nn_layer(w,x,b)

"""According to the output result, the value of b in the function cannot be directly viewed, but the return value can be viewed using  .numpy().
The following compares the performance of the graph and eager execution modes by performing the same operation (LSTM computation of one layer)

"""

# use the timeit module to measure the execution time of a small code segment

import timeit

# Create a convolutional layer
CNN_cell = tf.keras.layers.Conv2D(filters=100, kernel_size=2, strides=(1,1))

# Use @ tf.function to convert the operation into a graph
@tf.function
def CNN_fn(image):
    return CNN_cell(image)


image = tf.zeros([100, 200, 200, 3])

# Compare the execution time of the two modes

CNN_cell(image)
CNN_fn(image)

# Call timeit.timeit to measure the time required for executing the code 10 time
print("Time required for performing the computation of one convulutional neural network (CNN) layer in eager execution mode: ", timeit.timeit(lambda: CNN_cell(image), number=10))
print("Time Required for performing the computation of one CNN layer in graph mode: ", timeit.timeit(lambda: CNN_fn(image), number=10))

"""***NOTE:***
The comparison shows that the code execution efficiency in graph mode is much higher. Therefore, the @tf.function can be used to improve the code execution efficiency.
"""